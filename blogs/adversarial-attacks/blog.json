{
    "id": "adversarial-attacks",
    "slug": "adversarial-attacks",
    "title": "Adversarial Attacks Explained (And How to Defend ML Models Against Them)",
    "date": "Jan 28, 2026",
    "category": "Machine Learning",
    "author": "Sarthak Patel",
    "image": "./blogs/adversarial-attacks/Cover.png",
    "excerpt": "Artificial intelligence (AI) and machine learning (ML) systems can be fooled by adversarial attacks—deliberate changes to input data that lead to incorrect predictions.",
    "content": "<article class='blog-article'>\n\n    <h2>1. Introduction</h2>\n <p>Artificial intelligence (AI) and machine learning (ML) are becoming essential parts of contemporary technology, allowing for important applications in a wide range of fields. It has been shown that these systems can be fooled by adversarial attacks, which are deliberate changes to input data that can lead to highly confident but inaccurate predictions from machine learning models.</p>\n  <div class='blog-content-image'><img src='./blogs/adversarial-attacks/1.png' alt='Adversarial Attacks Concept' style='width: 100%; border-radius: var(--radius-lg); margin: 20px 0;'></div>\n   <p>Unlike traditional software defects or security exploits, adversarial attacks constitute a distinct class of vulnerabilities. They are especially difficult to fix because they take advantage of the underlying learning mechanisms of ML models rather than implementation errors.</p>\n\n    <h3>Definition and Fundamental Ideas</h3>\n    <p>Adversarial attacks occur when someone makes tiny changes to the input changes that are usually so small that humans can't even notice them. These attacks show that machine learning models can sometimes be very sensitive.</p>\n    <ul>\n        <li><strong>High-Dimensional Geometry:</strong> In high-dimensional spaces, even small changes across different features can add up to cause wrong predictions.</li>\n        <li><strong>Linear Behavior:</strong> Models often behave like simple linear systems in high dimensions, making them easy to fool.</li>\n        <li><strong>Overconfidence:</strong> Models tend to be overly confident even when wrong.</li>\n    </ul>\n\n    <h2>2. Objective</h2>\n    <p>The main objectives of this report are:</p>\n    <ul>\n        <li>To examine how machine learning models can be tricked.</li>\n        <li>To explore various types of adversarial attacks and methodologies.</li>\n        <li>To examine useful real-world examples with code.</li>\n        <li>To outline defense tactics.</li>\n    </ul>\n\n    <h2>3. Importance</h2>\n    <p>Studying adversarial attacks is crucial for Security Risks, Trust & Reliability, and Cybersecurity. Imagine a self-driving car misinterpreting a stop sign as a speed limit sign due to an adversarial attack—the consequences could be catastrophic.</p>\n\n    <h2>4. Applications and Domains Affected</h2>\n    <p>Adversarial attacks affect critical areas including:</p>\n    <ul>\n        <li><strong>Autonomous Vehicles:</strong> Misclassification of road signs.</li>\n        <li><strong>Healthcare:</strong> Incorrect diagnoses in medical imaging.</li>\n        <li><strong>Facial Recognition:</strong> Bypassing authentication systems.</li>\n        <li><strong>Financial Services:</strong> Evading fraud detection.</li>\n        <li><strong>Cybersecurity:</strong> Malware evading detection filters.</li>\n    </ul>\n\n    <h2>5. Types of Adversarial Attacks</h2>\n    <div class='blog-content-image'><img src='./blogs/adversarial-attacks/2.png' alt='Types of Attacks' style='width: 100%; border-radius: var(--radius-lg); margin: 20px 0;'></div>\n    <h3>5.1 White-box Attacks</h3>\n    <p>The attacker has full access to the model's architecture and parameters.</p>\n    <ul>\n        <li><strong>Fast Gradient Sign Method (FGSM):</strong> Adds perturbations in the direction of the gradient to maximize loss.</li>\n        <li><strong>Projected Gradient Descent (PGD):</strong> An iterative version of FGSM, considered one of the strongest first-order attacks.</li>\n    </ul>\n\n    <h3>5.2 Black-box Attacks</h3>\n    <p>The attacker has no access to internal workings and relies on query outputs.</p>\n\n    <h2>6. Real-World Examples (Code)</h2>\n    <p>Below is a simplified conceptual example of an FGSM attack using PyTorch:</p>\n    <pre><code class='language-python'>\nimport torch\nimport torch.nn.functional as F\n\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon * sign_data_grad\n    # Clip the perturbed image to stay within valid range [0,1]\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    return perturbed_image\n    </code></pre>\n\n    <h2>7. Evaluation and Metrics</h2>\n    <div class='blog-content-image'><img src='./blogs/adversarial-attacks/3.png' alt='Evaluation Metrics' style='width: 100%; border-radius: var(--radius-lg); margin: 20px 0;'></div>\n    <p><strong>Before Attack:</strong> The model correctly classifies images (e.g., 'Tiger Shark') with high confidence.</p>\n    <p><strong>After Attack:</strong> With adversarial noise added, the model misclassifies the same image (e.g., as 'Goldfish') while the image looks identical to the human eye. The confusion matrix shows a complete drop in accuracy on the adversarial examples.</p>\n\n    <h2>8. Conclusion</h2>\n    <p>Adversarial attacks pose a substantial threat to the reliability of deep learning systems. While models have achieved remarkable success, they remain fragile to subtle manipulations. Defending against these attacks via adversarial training and robust optimization is essential for safety-critical applications.</p>\n\n    <h2>9. References</h2>\n    <p>Inspired by the Advanced Threat Detection and Mitigation Bootcamp at SVNIT. References include works by Goodfellow et al. (2015), Kurakin et al. (2016), and Carlini & Wagner (2017).</p>\n\n</article>"
}
